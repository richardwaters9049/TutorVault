# backend/app/services/chat_service.py

# Import PyPDFLoader from langchain_community to load PDFs
from langchain_community.document_loaders import PyPDFLoader

# Import text splitter to break documents into manageable chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Import OpenAI embeddings from langchain_community for vector representation
from langchain_community.embeddings import OpenAIEmbeddings

# Import FAISS vectorstore from langchain_community to index and search document vectors
from langchain_community.vectorstores import FAISS

# Import RetrievalQA chain to perform question answering over documents
from langchain.chains import RetrievalQA

# Import ChatOpenAI from langchain_community as the language model for responses
from langchain_community.chat_models import ChatOpenAI

import tempfile


async def process_pdf_and_answer(file, question):
    """
    Process an uploaded PDF, split it into chunks, create embeddings,
    and answer a question based on the document's content.

    :param file: The uploaded PDF file (async file-like object)
    :param question: The question string to ask about the PDF content
    :return: The answer generated by the language model
    """

    # Step 1: Save the uploaded PDF to a temporary file to allow file system based processing
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        content = await file.read()  # Read PDF bytes asynchronously
        tmp.write(content)  # Write bytes to temp file
        tmp_path = tmp.name  # Path to the temp file

    # Step 2: Load the PDF and split it into smaller text chunks for processing
    loader = PyPDFLoader(tmp_path)
    documents = loader.load()  # Extract raw text from PDF

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.split_documents(documents)  # Split text into overlapping chunks

    # Step 3: Generate vector embeddings for chunks using OpenAI embeddings
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)  # Create FAISS index

    # Step 4: Create a RetrievalQA chain with a ChatOpenAI language model
    llm = ChatOpenAI(temperature=0.3)  # Language model with some variability
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # 'stuff' chain concatenates retrieved docs to answer question
        retriever=vectorstore.as_retriever(),
    )

    # Step 5: Run the QA chain on the user's question and return the answer
    result = qa_chain.run(question)
    return result
